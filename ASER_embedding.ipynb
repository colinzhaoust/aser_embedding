{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Database APIs for ASER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, install the necessay packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -r requirements.txt "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, install the aser package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python setup.py develop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Loading\n",
    "import numpy as np\n",
    "import random\n",
    "import json\n",
    "\n",
    "import keras.preprocessing.text as keras_text\n",
    "from keras import optimizers\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import sklearn.preprocessing as sklp\n",
    "from keras import backend as K\n",
    "\n",
    "import random, os, sys\n",
    "import numpy as np\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.callbacks import *\n",
    "from keras.initializers import *\n",
    "import tensorflow as tf\n",
    "from keras.engine.topology import Layer\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can import the necessay APIs to access the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from aser.database.db_API import KG_Connection\n",
    "# from aser.database.db_API import generate_event_id, generate_relation_id, generate_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We provide three different modes: *insert*, *cache*, and *memory*.\n",
    "* *insert* mode supports the insert operation.\n",
    "* *cache* mode load necessary metadata in the memory\n",
    "* *memory* mode loads the whole data in the memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, it still takes a lot of time, and costs a lot memory due to the large size.\n",
    "We use the *cache* mode to build a DB connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kg_conn = KG_Connection(db_path=r'./KG_v0.1.0.db', mode='cache')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('SIZE:', 'eventualities: ', len(kg_conn.event_id_set), 'relations:', len(kg_conn.relation_id_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Noted that eventualites extracted from differnent sentences are identified as the unique one in the database if their words are exactly the same, and relations between two eventualities are identified as the unique one distribution in the database. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can print the attributes of eventualites and relations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(list(zip(kg_conn.event_columns, kg_conn.event_column_types)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(list(zip(kg_conn.relation_columns, kg_conn.relation_column_types)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can retrieve eventualities from the database. \n",
    "The extractor is not released at present, we just use simple words to check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kg_conn.get_exact_match_event(generate_id('i learn python'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relations can also be retrieved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kg_conn.get_exact_match_relation([generate_id('i be tired'), generate_id('i sleep')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(kg_conn.get_exact_match_event(\"3df702fe2ef4b4084d63eac65fa3254f102172d3\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################\n",
    "## Step 1: Create a sample of KG   ##\n",
    "#####################################\n",
    "\n",
    "# # notice: it is a very sparse graph\n",
    "\n",
    "\n",
    "\n",
    "# # Sampling relations and corresponding events\n",
    "# sample_size = 1000\n",
    "# relation_id_list = list(kg_conn.relation_id_set)\n",
    "\n",
    "# sample_inds = list(range(len(relation_id_list)))\n",
    "# random.shuffle(sample_inds)\n",
    "\n",
    "# sample_ids = []\n",
    "# for ind in sample_inds[0:sample_size]:\n",
    "#     sample_ids.append(relation_id_list[ind])\n",
    "\n",
    "# sample_relations = dict()\n",
    "# sample_events = dict()\n",
    "\n",
    "\n",
    "\n",
    "# for sample_id in sample_ids:\n",
    "    \n",
    "#     relation = kg_conn.get_exact_match_relation(sample_id)\n",
    "    \n",
    "#     event1_id = relation[\"event1_id\"]\n",
    "#     event2_id = relation[\"event2_id\"]\n",
    "    \n",
    "#     event1 = kg_conn.get_exact_match_event(event1_id)\n",
    "#     event2 = kg_conn.get_exact_match_event(event2_id)\n",
    "    \n",
    "#     key  = event1[\"words\"] + \" && \" + event2[\"words\"]\n",
    "    \n",
    "#     sample_relations[key] = relation\n",
    "#     sample_events[event1_id] = event1\n",
    "#     sample_events[event2_id] = event2\n",
    "\n",
    "\n",
    "# print(\"# of relations: \", str(len(sample_relations.keys())))\n",
    "# print(\"# of event: \", str(len(sample_events.keys())))\n",
    "    \n",
    "# sample = dict()\n",
    "# sample[\"relation\"] = sample_relations\n",
    "# sample[\"events\"] = sample_events\n",
    "# # Dump to json for later use\n",
    "\n",
    "# with open('samples.json', 'w') as fp:\n",
    "#     json.dump(sample, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('samples.json') as fp:\n",
    "    samples = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "events = samples[\"events\"]\n",
    "relations = samples[\"relation\"]\n",
    "\n",
    "# treat events as nodes first\n",
    "event_list = list(events.values())\n",
    "\n",
    "event_id_list = []\n",
    "\n",
    "for event in event_list:\n",
    "    event_id_list.append(event[\"_id\"])\n",
    "    \n",
    "# prepare the edge list\n",
    "edge_list = []\n",
    "\n",
    "for relation in relations.values():\n",
    "    edge = []\n",
    "    event1_id = relation[\"event1_id\"]\n",
    "    event2_id = relation[\"event2_id\"]\n",
    "    weight = relation['Co_Occurrence']\n",
    "    \n",
    "    edge.append(event_id_list.index(event1_id))\n",
    "    edge.append(event_id_list.index(event2_id))\n",
    "    edge.append(weight)\n",
    "    \n",
    "    edge_list.append(edge)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################\n",
    "##           Step 2: Adapted Skipgram + Deepwalk         ##\n",
    "###########################################################\n",
    "\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import random\n",
    "\n",
    "\n",
    "class Graph():\n",
    "    def __init__(self, nx_G, is_directed, p, q):\n",
    "        self.G = nx_G\n",
    "        self.is_directed = is_directed\n",
    "        self.p = p\n",
    "        self.q = q\n",
    "\n",
    "    def node2vec_walk(self, walk_length, start_node):\n",
    "        '''\n",
    "        Simulate a random walk starting from start node.\n",
    "        '''\n",
    "        G = self.G\n",
    "        alias_nodes = self.alias_nodes\n",
    "        alias_edges = self.alias_edges\n",
    "\n",
    "        walk = [start_node]\n",
    "\n",
    "        while len(walk) < walk_length:\n",
    "            cur = walk[-1]\n",
    "            cur_nbrs = sorted(G.neighbors(cur))\n",
    "            if len(cur_nbrs) > 0:\n",
    "                if len(walk) == 1:\n",
    "                    walk.append(cur_nbrs[alias_draw(alias_nodes[cur][0], alias_nodes[cur][1])])\n",
    "                else:\n",
    "                    prev = walk[-2]\n",
    "                    next = cur_nbrs[alias_draw(alias_edges[(prev, cur)][0],\n",
    "                                               alias_edges[(prev, cur)][1])]\n",
    "                    walk.append(next)\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        return walk\n",
    "\n",
    "    def simulate_walks(self, num_walks, walk_length):\n",
    "        '''\n",
    "        Repeatedly simulate random walks from each node.\n",
    "        '''\n",
    "        G = self.G\n",
    "        walks = []\n",
    "        nodes = list(G.nodes())\n",
    "        print('Walk iteration:')\n",
    "        for walk_iter in range(num_walks):\n",
    "            print(str(walk_iter + 1), '/', str(num_walks))\n",
    "            random.shuffle(nodes)\n",
    "            for node in nodes:\n",
    "                walks.append(self.node2vec_walk(walk_length=walk_length, start_node=node))\n",
    "\n",
    "        return walks\n",
    "\n",
    "    def get_alias_edge(self, src, dst):\n",
    "        '''\n",
    "        Get the alias edge setup lists for a given edge.\n",
    "        '''\n",
    "        G = self.G\n",
    "        p = self.p\n",
    "        q = self.q\n",
    "\n",
    "        unnormalized_probs = []\n",
    "        for dst_nbr in sorted(G.neighbors(dst)):\n",
    "            if dst_nbr == src:\n",
    "                unnormalized_probs.append(G[dst][dst_nbr]['weight'] / p)\n",
    "            elif G.has_edge(dst_nbr, src):\n",
    "                unnormalized_probs.append(G[dst][dst_nbr]['weight'])\n",
    "            else:\n",
    "                unnormalized_probs.append(G[dst][dst_nbr]['weight'] / q)\n",
    "        norm_const = sum(unnormalized_probs)\n",
    "        normalized_probs = [float(u_prob) / norm_const for u_prob in unnormalized_probs]\n",
    "\n",
    "        return alias_setup(normalized_probs)\n",
    "\n",
    "    def preprocess_transition_probs(self):\n",
    "        '''\n",
    "        Preprocessing of transition probabilities for guiding the random walks.\n",
    "        '''\n",
    "        G = self.G\n",
    "        is_directed = self.is_directed\n",
    "\n",
    "        alias_nodes = {}\n",
    "        for node in G.nodes():\n",
    "            unnormalized_probs = [G[node][nbr]['weight'] for nbr in sorted(G.neighbors(node))]\n",
    "            norm_const = sum(unnormalized_probs)\n",
    "            normalized_probs = [float(u_prob) / norm_const for u_prob in unnormalized_probs]\n",
    "            alias_nodes[node] = alias_setup(normalized_probs)\n",
    "\n",
    "        alias_edges = {}\n",
    "        triads = {}\n",
    "\n",
    "        if is_directed:\n",
    "            for edge in G.edges():\n",
    "                alias_edges[edge] = self.get_alias_edge(edge[0], edge[1])\n",
    "        else:\n",
    "            for edge in G.edges():\n",
    "                alias_edges[edge] = self.get_alias_edge(edge[0], edge[1])\n",
    "                alias_edges[(edge[1], edge[0])] = self.get_alias_edge(edge[1], edge[0])\n",
    "\n",
    "        self.alias_nodes = alias_nodes\n",
    "        self.alias_edges = alias_edges\n",
    "\n",
    "        return\n",
    "\n",
    "\n",
    "def alias_setup(probs):\n",
    "    '''\n",
    "    Compute utility lists for non-uniform sampling from discrete distributions.\n",
    "    Refer to https://hips.seas.harvard.edu/blog/2013/03/03/the-alias-method-efficient-sampling-with-many-discrete-outcomes/\n",
    "    for details\n",
    "    '''\n",
    "    K = len(probs)\n",
    "    q = np.zeros(K)\n",
    "    J = np.zeros(K, dtype=np.int)\n",
    "\n",
    "    smaller = []\n",
    "    larger = []\n",
    "    for kk, prob in enumerate(probs):\n",
    "        q[kk] = K * prob\n",
    "        if q[kk] < 1.0:\n",
    "            smaller.append(kk)\n",
    "        else:\n",
    "            larger.append(kk)\n",
    "\n",
    "    while len(smaller) > 0 and len(larger) > 0:\n",
    "        small = smaller.pop()\n",
    "        large = larger.pop()\n",
    "\n",
    "        J[small] = large\n",
    "        q[large] = q[large] + q[small] - 1.0\n",
    "        if q[large] < 1.0:\n",
    "            smaller.append(large)\n",
    "        else:\n",
    "            larger.append(large)\n",
    "\n",
    "    return J, q\n",
    "\n",
    "\n",
    "def alias_draw(J, q):\n",
    "    '''\n",
    "    Draw sample from a non-uniform discrete distribution using alias sampling.\n",
    "    '''\n",
    "    K = len(J)\n",
    "\n",
    "    kk = int(np.floor(np.random.rand() * K))\n",
    "    if np.random.rand() < q[kk]:\n",
    "        return kk\n",
    "    else:\n",
    "        return J[kk]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Walk iteration:\n",
      "1 / 10\n",
      "2 / 10\n",
      "3 / 10\n",
      "4 / 10\n",
      "5 / 10\n",
      "6 / 10\n",
      "7 / 10\n",
      "8 / 10\n",
      "9 / 10\n",
      "10 / 10\n"
     ]
    }
   ],
   "source": [
    "# Prepare the walks\n",
    "\n",
    "def get_G_from_edges(edges):\n",
    "    edge_dict = dict()\n",
    "    # calculate the count for all the edges\n",
    "    for edge in edges:\n",
    "        edge_key = str(edge[0]) + '_' + str(edge[1])\n",
    "        if edge_key not in edge_dict:\n",
    "            edge_dict[edge_key] = edge[2]\n",
    "        else:\n",
    "            edge_dict[edge_key] += edge[2]\n",
    "    tmp_G = nx.DiGraph()\n",
    "    \n",
    "    for edge_key in edge_dict:\n",
    "        weight = edge_dict[edge_key]\n",
    "        # add edges to the graph\n",
    "        tmp_G.add_edge(edge_key.split('_')[0], edge_key.split('_')[1])\n",
    "        # add weights for all the edges\n",
    "        tmp_G[edge_key.split('_')[0]][edge_key.split('_')[1]]['weight'] = weight\n",
    "    return tmp_G\n",
    "\n",
    "directed = True\n",
    "p = 1\n",
    "q = 1\n",
    "num_walks = 10\n",
    "walk_length = 10\n",
    "dimension = 200\n",
    "iterations = 50\n",
    "\n",
    "train_edges = edge_list\n",
    "\n",
    "G = Graph(get_G_from_edges(train_edges), directed, p, q)\n",
    "G.preprocess_transition_probs()\n",
    "walks = G.simulate_walks(num_walks, walk_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# test skipgram\n",
    "\n",
    "# couples, labels = sequence.skipgrams(np.arange(3), vocabulary_size=3)\n",
    "# for couple in couples:\n",
    "#     assert couple[0] in [0, 1, 2] and couple[1] in [0, 1, 2]\n",
    "\n",
    "# # test window size and categorical labels\n",
    "# data = np.array([np.arange(5), np.arange(5)])\n",
    "\n",
    "# print(numerical_walks[0])\n",
    "# couples, labels = sequence.skipgrams(numerical_walks[0], vocabulary_size=2, window_size=1,\n",
    "#                             categorical=True)\n",
    "# print(couples)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Training Data\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import keras.preprocessing.sequence as sequence\n",
    "\n",
    "\n",
    "def event2word(event_id_list,event_list_original, tokenizer):\n",
    "    event_word = []\n",
    "    for index in event_id_list:\n",
    "        event = event_list_original[index]\n",
    "        event_word.append(event[\"words\"])\n",
    "        \n",
    "    event_word = tokenizer.texts_to_sequences(event_word)\n",
    "    event_word = pad_sequences(event_word)\n",
    "    \n",
    "    return event_word\n",
    "\n",
    "\n",
    "max_features = 1790 # how many unique words to use (i.e num rows in embedding vector)\n",
    "maxlen = 10 # max number of words of a event\n",
    "embed_size = 300 # how big is each word vector\n",
    "event_size = 1000 # number of eventualities invovled\n",
    "vocab_size = max_features\n",
    "features_dim = embed_size \n",
    "\n",
    "\n",
    "# event list -  edge_lsit\n",
    "window_size = 5\n",
    "event_size = len(event_list)\n",
    "\n",
    "# collect all the words in the events\n",
    "event_words_collection = []\n",
    "for event in event_list:\n",
    "    event_words_collection.append(event[\"words\"])\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(event_words_collection)\n",
    "\n",
    "numerical_walks = []\n",
    "for walk in walks:\n",
    "    if len(walk) > 1:\n",
    "        numerical_walks.append(list(map(int, walk)))\n",
    "\n",
    "# sampling_table = sequence.make_sampling_table(event_size)\n",
    "couples = []\n",
    "labels = []\n",
    "\n",
    "# temp_walk = numerical_walks\n",
    "# np.shuffle(temp_walk)\n",
    "# sample_walk = temp_walk[0:2000]\n",
    "\n",
    "\n",
    "for walk in numerical_walks:\n",
    "    temp_couples, temp_labels = sequence.skipgrams(walk, event_size, window_size=5)\n",
    "    couples.extend(temp_couples)\n",
    "    labels.extend(temp_labels)\n",
    "    \n",
    "labels = np.array(labels, dtype=\"int32\")\n",
    "\n",
    "event_target, event_context = zip(*couples)\n",
    "target_input = event2word(event_target, event_list, tokenizer)\n",
    "context_input = event2word(event_context, event_list, tokenizer)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xzhao\\Anaconda3\\envs\\Keras\\lib\\site-packages\\ipykernel_launcher.py:7: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "# preparing embedding matrix\n",
    "\n",
    "EMBEDDING_FILE = 'glove.840B.300d.txt'\n",
    "def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE, encoding='utf-8'))\n",
    "\n",
    "all_embs = np.stack(embeddings_index.values())\n",
    "emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "embed_size = all_embs.shape[1]\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "nb_words = min(max_features, len(word_index))\n",
    "embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "for word, i in word_index.items():\n",
    "    if i >= (max_features): continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None: embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipgramModel:\n",
    "    def __init__(self, max_features, embed_size, embedding_matrix):\n",
    "        vocab_size = max_features\n",
    "        features_dim = embed_size\n",
    "        \n",
    "        target_input = Input(shape=(None,), name='target_input')\n",
    "        context_input = Input(shape=(None,), name='context_input')\n",
    "        \n",
    "        #embedding_layer = Embedding(vocab_size, features_dim, input_length=1, name='embedding_layer')\n",
    "        \n",
    "        embedding_layer = Embedding(vocab_size, features_dim, weights=[embedding_matrix])\n",
    "        \n",
    "        #encode target words\n",
    "        target_encoded = embedding_layer(target_input)\n",
    "        # target_encoded = Reshape((features_dim, 1))(target_encoded)\n",
    "        target_encoded = Bidirectional(CuDNNLSTM(128, return_sequences=False))(target_encoded)\n",
    "\n",
    "        #encode context words   \n",
    "        context_encoded = embedding_layer(context_input)\n",
    "        # context_encoded = Reshape((features_dim, 1))(context_encoded)\n",
    "        context_encoded = Bidirectional(CuDNNLSTM(128, return_sequences=False))(context_encoded)\n",
    "        \n",
    "        #dot product two seqs\n",
    "        \n",
    "        dot_product = dot([target_encoded, context_encoded], axes=1)\n",
    "        \n",
    "        # dot_product = Reshape((1,))(dot_product)\n",
    "\n",
    "        #normalize\n",
    "        output_layer = Dense(1, activation='sigmoid', name='output_layer')(dot_product)\n",
    "        \n",
    "        self.main_model = Model(inputs=[target_input, context_input], outputs=[output_layer])\n",
    "\n",
    "        similarity = dot([target_encoded, context_encoded], axes=0, normalize=True)\n",
    "        self.embedding_model = Model(inputs=[target_input, context_input], outputs=[target_encoded])\n",
    "        self.similarity_model = Model(inputs=[target_input, context_input], outputs=[similarity])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      " - 22s - loss: 0.3479 - acc: 0.8246\n",
      "Epoch 2/2\n",
      " - 20s - loss: 0.0706 - acc: 0.9767\n"
     ]
    }
   ],
   "source": [
    "skipgram = SkipgramModel(vocab_size, features_dim, embedding_matrix)\n",
    "skipgram.main_model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "skipgram.main_model.fit([target_input, context_input], labels,epochs=2, verbose=2)\n",
    "skipgram.main_model.save_weights('skipgram_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the emebdding vectors\n",
    "skipgram = SkipgramModel(vocab_size, features_dim, embedding_matrix)\n",
    "skipgram.main_model.load_weights('skipgram_weights.h5')\n",
    "\n",
    "out = skipgram.embedding_model.predict_on_batch([target_input[:2],context_input[:2]])\n",
    "\n",
    "# event_list\n",
    "# event_id_list\n",
    "# edge_list\n",
    "\n",
    "embedding_dic = dict()\n",
    "\n",
    "temp = np.array(range(len(event_list)))\n",
    "out = skipgram.embedding_model.predict_on_batch([temp,temp])\n",
    "\n",
    "for i in range(len(out)):\n",
    "    embedding_dic[event_id_list[i]] = list(out[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tmp_accuracy: 0.6323895\n"
     ]
    }
   ],
   "source": [
    "###########################################################\n",
    "##                     Step 3: Evalution                 ##\n",
    "###########################################################\n",
    "\n",
    "# Link Prediction\n",
    "def randomly_choose_false_edges(nodes, true_edges):\n",
    "    tmp_list = list()\n",
    "    all_edges = list()\n",
    "    for i in range(len(nodes)):\n",
    "        for j in range(len(nodes)):\n",
    "            all_edges.append((i, j))\n",
    "    random.shuffle(all_edges)\n",
    "    for edge in all_edges:\n",
    "        if edge[0] == edge[1]:\n",
    "            continue\n",
    "        if (nodes[edge[0]], nodes[edge[1]]) not in true_edges and (nodes[edge[1]], nodes[edge[0]]) not in true_edges:\n",
    "            tmp_list.append((nodes[edge[0]], nodes[edge[1]]))\n",
    "    return tmp_list\n",
    "\n",
    "\n",
    "# Compare with original score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def get_AUC(embedding_dic, true_edges, false_edges, event_id_list):\n",
    "    true_list = list()\n",
    "    prediction_list = list()\n",
    "    for edge in true_edges:\n",
    "        tmp_score = get_neighbourhood_score(embedding_dic, event_id_list[edge[0]], event_id_list[edge[1]])\n",
    "        true_list.append(1)\n",
    "        prediction_list.append(tmp_score)\n",
    "\n",
    "    for edge in false_edges:\n",
    "        tmp_score = get_neighbourhood_score(embedding_dic, event_id_list[edge[0]], event_id_list[edge[1]])\n",
    "        true_list.append(0)\n",
    "        prediction_list.append(tmp_score)\n",
    "        \n",
    "    y_true = np.array(true_list)\n",
    "    y_scores = np.array(prediction_list)\n",
    "    return roc_auc_score(y_true, y_scores)\n",
    "\n",
    "def get_neighbourhood_score(embedding_dic, node1_id, node2_id):\n",
    "    try:\n",
    "        vector1 = embedding_dic[node1_id]\n",
    "        vector2 = embedding_dic[node2_id]\n",
    "        return np.dot(vector1, vector2) / (np.linalg.norm(vector1) * np.linalg.norm(vector2))\n",
    "    except:\n",
    "        return random.random()\n",
    "    \n",
    "# event_list\n",
    "# edge_list\n",
    "mask_event_list = range(len(event_list))\n",
    "mask_edge_list = []\n",
    "\n",
    "for i in range(len(edge_list)):\n",
    "    mask_edge_list = edge_list[:2]\n",
    "\n",
    "negative_edges = randomly_choose_false_edges(mask_event_list, mask_edge_list)[:1000]\n",
    "tmp_AUC_score = get_AUC(embedding_dic, edge_list, negative_edges,event_id_list)\n",
    "print('tmp_accuracy:', tmp_AUC_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2]\n"
     ]
    }
   ],
   "source": [
    "a = [1,2,3]\n",
    "b = a[:2]\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
